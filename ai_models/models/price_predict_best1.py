import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from datetime import datetime
from sklearn.preprocessing import MinMaxScaler, RobustScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Concatenate, BatchNormalization, Multiply, MultiHeadAttention, Layer, TimeDistributed, Lambda, Conv1D
from tensorflow.keras.optimizers import Adam, AdamW, RMSprop, Nadam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
import random
import math
import matplotlib.font_manager as fm
import matplotlib.dates as mdates
from ta.trend import SMAIndicator, MACD
from ta.momentum import RSIIndicator, ROCIndicator
from ta.volatility import BollingerBands
import os
import pickle
import json
import yfinance as yf
import requests
from bs4 import BeautifulSoup
import logging
from ai_models.database.db_config import execute_query, execute_values_query, execute_transaction

# TensorFlow ë¡œê¹… ì„¤ì •
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0=all, 1=no info, 2=no warnings, 3=no errors
logging.getLogger('tensorflow').setLevel(logging.ERROR)

# GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸ ë° ìµœì í™”
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    print(f"GPU ì‚¬ìš© ê°€ëŠ¥: {gpus[0]}")
    try:
        # GPU ë©”ëª¨ë¦¬ ì„¤ì • (ë©”ëª¨ë¦¬ ì„±ì¥ ëŒ€ì‹  ê³ ì •ëœ ë©”ëª¨ë¦¬ í• ë‹¹ ì‚¬ìš©)
        for gpu in gpus:
            # ë©”ëª¨ë¦¬ ì œí•œ ì„¤ì • (90% ì‚¬ìš©)
            tf.config.experimental.set_virtual_device_configuration(
                gpu,
                [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024*9)]  # 9GB
            )
    except RuntimeError as e:
        print(f"GPU ë©”ëª¨ë¦¬ ì„¤ì • ì¤‘ ì˜¤ë¥˜: {e}")
else:
    print("GPUë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPUë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")

print("TensorFlow ë²„ì „:", tf.__version__)

# í•œê¸€ í°íŠ¸ ì„¤ì •
try:
    font_list = [f.name for f in fm.fontManager.ttflist]
    for font in ['NanumBarunGothic', 'NanumGothic', 'Malgun Gothic', 'Gulim']:
        if font in font_list:
            plt.rcParams['font.family'] = font
            print(f"í•œê¸€ í°íŠ¸ '{font}' ì‚¬ìš©")
            break
    else:
        print("í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ ì‚¬ìš©")

    plt.rcParams['axes.unicode_minus'] = False
except Exception as e:
    print(f"í°íŠ¸ ì„¤ì • ì˜¤ë¥˜: {e}")

# ì¬í˜„ì„± ì„¤ì • ê°•í™”
SEED = 42
os.environ['PYTHONHASHSEED'] = str(SEED)
os.environ['TF_DETERMINISTIC_OPS'] = '1'
os.environ['TF_CUDNN_DETERMINISTIC'] = '1'

# ëª¨ë“  ëœë¤ ì‹œë“œ ì„¤ì •
np.random.seed(SEED)
tf.random.set_seed(SEED)
random.seed(SEED)

# ë°°ì¹˜ í¬ê¸° ì¦ê°€ (GPU ë©”ëª¨ë¦¬ì— ë§ê²Œ ì¡°ì •)
BATCH_SIZE = 128  # 32ì—ì„œ 128ë¡œ ì¦ê°€

def create_predictions_table():
    """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì €ì¥í•  í…Œì´ë¸” ìƒì„±"""
    queries = [
        ("""
        CREATE TABLE IF NOT EXISTS price_predictions (
            id SERIAL PRIMARY KEY,
            stock_code VARCHAR(10) NOT NULL,
            stock_name VARCHAR(50) NOT NULL,
            prediction_date TIMESTAMPTZ NOT NULL,
            target_date TIMESTAMPTZ NOT NULL,
            predicted_price DECIMAL(10,2) NOT NULL,
            actual_price DECIMAL(10,2),
            prediction_error DECIMAL(10,2),
            created_at TIMESTAMPTZ DEFAULT CURRENT_TIMESTAMP
        );
        """, None),
        ("CREATE INDEX IF NOT EXISTS idx_price_predictions_date ON price_predictions (prediction_date, target_date);", None),
        ("CREATE INDEX IF NOT EXISTS idx_price_predictions_stock ON price_predictions (stock_code);", None)
    ]
    execute_transaction(queries)
    print("Price predictions table created successfully!")

def load_data_from_db():
    """ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ"""
    print("Loading stock data...")
    try:
        # ì£¼ê°€ ë°ì´í„° ë¡œë“œ
        query = """
        SELECT 
            time, stock_code, stock_name,
            open_price, high_price, low_price, close_price,
            volume, market_cap, foreign_holding, foreign_holding_ratio
        FROM stock_prices
        WHERE stock_name = 'LGì „ì'
        ORDER BY time;
        """
        stock_data = pd.DataFrame(execute_query(query))
        
        # ê°ì„± ë°ì´í„° ë¡œë“œ
        query = """
        SELECT 
            pub_date, title,
            finbert_positive, finbert_negative, finbert_neutral,
            finbert_sentiment
        FROM news_sentiment
        ORDER BY pub_date;
        """
        sentiment_data = pd.DataFrame(execute_query(query))
        
        # ê²½ì œì§€í‘œ ë°ì´í„° ë¡œë“œ
        query = """
        SELECT 
            time,
            treasury_10y, dollar_index, usd_krw, korean_bond_10y
        FROM economic_indicators
        ORDER BY time;
        """
        economic_data = pd.DataFrame(execute_query(query))
        
        print("Stock data shape:", stock_data.shape)
        print("Sentiment data shape:", sentiment_data.shape)
        print("Economic data shape:", economic_data.shape)
        
        return stock_data, sentiment_data, economic_data
        
    except Exception as e:
        print(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        raise

def save_prediction(stock_code, stock_name, prediction_date, target_date, predicted_price, actual_price=None):
    """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥"""
    prediction_error = None
    if actual_price is not None:
        prediction_error = predicted_price - actual_price
    
    query = """
    INSERT INTO price_predictions (
        stock_code, stock_name, prediction_date, target_date,
        predicted_price, actual_price, prediction_error
    ) VALUES (%s, %s, %s, %s, %s, %s, %s)
    """
    params = (
        stock_code, stock_name, prediction_date, target_date,
        predicted_price, actual_price, prediction_error
    )
    execute_query(query, params, fetch=False)

# 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
print("Loading stock data...")
try:
    # ë°ì´í„° ë¡œë”© ì‹œ ì •ë ¬ ë³´ì¥
    stock_data = pd.read_csv('/kaggle/input/dataset/kospi200_stock_prices_pykrx_20231107_20250321_20250407.csv')
    stock_data = stock_data.sort_values(['ì¢…ëª©ëª…', 'ê¸°ì¤€ì¼ì']).reset_index(drop=True)

    print("Stock data columns:", stock_data.columns.tolist())
    print("Stock data shape:", stock_data.shape)
    print("Stock data head:\n", stock_data.head())
except Exception as e:
    print(f"ì£¼ì‹ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    raise

print("\nLoading sentiment data...")
try:
    # ê°ì„± ë°ì´í„°ë„ ì •ë ¬ ë³´ì¥
    sentiment_data = pd.read_excel('/kaggle/input/dataset/lg_news_finbert_sentiment.xlsx')
    sentiment_data = sentiment_data.sort_values('PubDate').reset_index(drop=True)

    print("Sentiment data columns:", sentiment_data.columns.tolist())
    print("Sentiment data shape:", sentiment_data.shape)
    print("Sentiment data head:\n", sentiment_data.head())
except Exception as e:
    print(f"ê°ì„± ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    raise

print("\nLoading economic indicators data...")
try:
    # ë¯¸êµ­ 10ë…„ë¬¼ êµ­ì±„ ê¸ˆë¦¬
    treasury = yf.download('^TNX', start='2023-11-07', end='2025-03-21')
    treasury = treasury[['Close']].rename(columns={'Close': 'treasury_10y'})
    
    # ë‹¬ëŸ¬ ì¸ë±ìŠ¤
    dollar_index = yf.download('DX-Y.NYB', start='2023-11-07', end='2025-03-21')
    dollar_index = dollar_index[['Close']].rename(columns={'Close': 'dollar_index'})
    
    # ì›ë‹¬ëŸ¬ í™˜ìœ¨
    usdkrw = yf.download('USDKRW=X', start='2023-11-07', end='2025-03-21')
    usdkrw = usdkrw[['Close']].rename(columns={'Close': 'usd_krw'})
    
    # í•œêµ­ 10ë…„ë¬¼ êµ­ì±„ ê¸ˆë¦¬ (yfinance ì‚¬ìš©)
    korean_bond = yf.download('KR10YT=RR', start='2023-11-07', end='2025-03-21')
    korean_bond = korean_bond[['Close']].rename(columns={'Close': 'korean_bond_10y'})
    
    # ëª¨ë“  ê²½ì œì§€í‘œ ë³‘í•©
    economic_data = pd.concat([treasury, dollar_index, usdkrw, korean_bond], axis=1)
    
    # MultiIndex ë¬¸ì œ í•´ê²°
    if isinstance(economic_data.columns, pd.MultiIndex):
        economic_data.columns = economic_data.columns.get_level_values(0)
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    economic_data = economic_data.ffill().bfill()
    
    print("Economic indicators data columns:", economic_data.columns.tolist())
    print("Economic indicators data shape:", economic_data.shape)
    print("Economic indicators data head:\n", economic_data.head())
except Exception as e:
    print(f"ê²½ì œì§€í‘œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
    raise

# LGì „ì ë°ì´í„°ë§Œ í•„í„°ë§
lg_data = stock_data[stock_data['ì¢…ëª©ëª…'] == 'LGì „ì'].copy()
print("\nLG data shape:", lg_data.shape)
print("LG data head:\n", lg_data.head())

# ë‚ ì§œ í˜•ì‹ ë³€í™˜
lg_data['ê¸°ì¤€ì¼ì'] = pd.to_datetime(lg_data['ê¸°ì¤€ì¼ì'])
sentiment_data['PubDate'] = pd.to_datetime(sentiment_data['PubDate'])
economic_data.index = pd.to_datetime(economic_data.index)

# ë°ì´í„° ë³‘í•©
merged_data = pd.merge(lg_data, sentiment_data, left_on='ê¸°ì¤€ì¼ì', right_on='PubDate', how='left')
merged_data = pd.merge(merged_data, economic_data, left_on='ê¸°ì¤€ì¼ì', right_index=True, how='left')
print("\nMerged data shape:", merged_data.shape)

# ê¸°ìˆ ì  ì§€í‘œ ì¶”ê°€
def add_technical_indicators(df):
    # RSI
    rsi = RSIIndicator(close=df['í˜„ì¬ê°€'], window=14)
    df['RSI'] = rsi.rsi()

    # MACD
    macd = MACD(close=df['í˜„ì¬ê°€'])
    df['MACD'] = macd.macd()
    df['MACD_SIGNAL'] = macd.macd_signal()
    df['MACD_HIST'] = macd.macd_diff()

    # ë³¼ë¦°ì € ë°´ë“œ
    bbands = BollingerBands(close=df['í˜„ì¬ê°€'], window=20)
    df['BB_UPPER'] = bbands.bollinger_hband()
    df['BB_MIDDLE'] = bbands.bollinger_mavg()
    df['BB_LOWER'] = bbands.bollinger_lband()
    df['BB_PERCENT'] = (df['í˜„ì¬ê°€'] - df['BB_LOWER']) / (df['BB_UPPER'] - df['BB_LOWER'])

    # ì´ë™í‰ê· 
    df['MA5'] = SMAIndicator(close=df['í˜„ì¬ê°€'], window=5).sma_indicator()
    df['MA20'] = SMAIndicator(close=df['í˜„ì¬ê°€'], window=20).sma_indicator()
    df['MA60'] = SMAIndicator(close=df['í˜„ì¬ê°€'], window=60).sma_indicator()

    # ê±°ë˜ëŸ‰ ì§€í‘œ
    df['VOLUME_MA5'] = SMAIndicator(close=df['ê±°ë˜ëŸ‰'], window=5).sma_indicator()
    df['VOLUME_MA20'] = SMAIndicator(close=df['ê±°ë˜ëŸ‰'], window=20).sma_indicator()
    df['VOLUME_RATIO'] = df['ê±°ë˜ëŸ‰'] / df['VOLUME_MA20']

    # ëª¨ë©˜í…€ ì§€í‘œ
    df['MOM'] = df['í˜„ì¬ê°€'].diff(10)
    df['ROC'] = ROCIndicator(close=df['í˜„ì¬ê°€'], window=10).roc()

    return df

# ê¸°ìˆ ì  ì§€í‘œ ì¶”ê°€
merged_data = add_technical_indicators(merged_data)

# ê²°ì¸¡ì¹˜ ì²˜ë¦¬
merged_data = merged_data.fillna(method='ffill').fillna(method='bfill').fillna(0)

# ë°ì´í„° ì „ì²˜ë¦¬ ê°œì„ 
def enhanced_preprocessing(df):
    # ê°€ê²© ë³€ë™ë¥  ê³„ì‚°
    df['price_change'] = df['í˜„ì¬ê°€'].pct_change()
    df['price_volatility'] = df['price_change'].rolling(window=5).std()
    
    # ê±°ë˜ëŸ‰ ë³€ë™ë¥ 
    df['volume_change'] = df['ê±°ë˜ëŸ‰'].pct_change()
    df['volume_volatility'] = df['volume_change'].rolling(window=5).std()
    
    # ê°€ê²© ëª¨ë©˜í…€
    df['price_momentum'] = df['í˜„ì¬ê°€'] / df['í˜„ì¬ê°€'].rolling(window=5).mean() - 1
    
    # ê±°ë˜ëŸ‰ ëª¨ë©˜í…€
    df['volume_momentum'] = df['ê±°ë˜ëŸ‰'] / df['ê±°ë˜ëŸ‰'].rolling(window=5).mean() - 1
    
    # ê°€ê²© ë³€ë™ ì¶”ì„¸
    df['price_trend'] = df['í˜„ì¬ê°€'].rolling(window=5).apply(lambda x: np.polyfit(range(len(x)), x, 1)[0])
    
    # ì´ìƒì¹˜ ì²˜ë¦¬ (IQR ë°©ë²•)
    for col in ['í˜„ì¬ê°€', 'ê±°ë˜ëŸ‰', 'price_change', 'volume_change']:
        q1 = df[col].quantile(0.25)
        q3 = df[col].quantile(0.75)
        iqr = q3 - q1
        lower_bound = q1 - 1.5 * iqr
        upper_bound = q3 + 1.5 * iqr
        df[col] = df[col].clip(lower_bound, upper_bound)
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ìµœì‹  pandas ë°©ì‹)
    df = df.ffill().bfill()
    
    # ê°ì„± ë°ì´í„° ë³´ê°„
    sentiment_cols = ['finbert_positive', 'finbert_negative', 'finbert_neutral']
    for col in sentiment_cols:
        if col in df.columns:
            # ê°ì„± ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë³´ê°„
            mask = df[col] != 0
            if mask.any():
                df[col] = df[col].interpolate(method='linear')
    
    # ê²½ì œ ì§€í‘œ ë³´ê°„
    economic_cols = ['treasury_10y', 'dollar_index', 'usd_krw', 'korean_bond_10y']
    for col in economic_cols:
        if col in df.columns:
            # ê²½ì œ ì§€í‘œê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë³´ê°„
            mask = df[col] != 0
            if mask.any():
                df[col] = df[col].interpolate(method='linear')
    
    return df

# ë°ì´í„° ì „ì²˜ë¦¬ ì ìš©
merged_data = enhanced_preprocessing(merged_data)

# ìŠ¤ì¼€ì¼ë§ í´ë˜ìŠ¤ ê°œì„ 
class EnhancedPriceScaler:
    def __init__(self):
        self.price_scaler = MinMaxScaler(feature_range=(0.1, 0.9))  # 0-1 ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ ì¡°ì •
        self.feature_scaler = MinMaxScaler(feature_range=(0.1, 0.9))

    def fit_transform(self, data, price_cols):
        data_copy = data.copy()
        
        # ë¬¸ìì—´ ì»¬ëŸ¼ê³¼ ë‚ ì§œ ì»¬ëŸ¼ ì œì™¸
        exclude_cols = ['ê¸°ì¤€ì¼ì', 'ì¢…ëª©ì½”ë“œ', 'ì¢…ëª©ëª…', 'Title', 'PubDate', 'finbert_sentiment']
        for col in exclude_cols:
            if col in data_copy.columns:
                data_copy = data_copy.drop(columns=[col])
        
        # ê°€ê²© ë°ì´í„°ì™€ ë‹¤ë¥¸ íŠ¹ì„± ë¶„ë¦¬
        price_data = data_copy[price_cols]
        other_data = data_copy.drop(columns=price_cols)
        
        # ê°ê° ìŠ¤ì¼€ì¼ë§
        scaled_price = self.price_scaler.fit_transform(price_data)
        scaled_other = self.feature_scaler.fit_transform(other_data)
        
        # ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ê²°í•©
        scaled_data = np.concatenate([scaled_price, scaled_other], axis=1)
        scaled_df = pd.DataFrame(scaled_data, columns=price_cols + other_data.columns.tolist())
        
        # ì›ë˜ ì»¬ëŸ¼ ìˆœì„œ ë³µì›
        scaled_df = scaled_df[data_copy.columns]
        
        return scaled_df

    def inverse_transform_price(self, scaled_price):
        if len(scaled_price.shape) == 1:
            scaled_price = scaled_price.reshape(-1, 1)
        
        dummy_data = np.zeros((scaled_price.shape[0], len(self.price_scaler.feature_names_in_)))
        dummy_data[:, 0] = scaled_price.flatten()
        
        unscaled = self.price_scaler.inverse_transform(dummy_data)
        return unscaled[:, 0]

# ì†ì‹¤ í•¨ìˆ˜ ê°œì„ 
def enhanced_weighted_time_mse(y_true, y_pred):
    # ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ ìœ„í•œ ì‘ì€ ê°’ ì¶”ê°€
    epsilon = 1e-7
    
    # ì‹œê°„ ê°€ì¤‘ì¹˜ ì¡°ì • (ì²«ë‚  ê°€ì¤‘ì¹˜ ê°•í™”)
    time_weights = tf.constant([0.6, 0.2, 0.1, 0.07, 0.03], dtype=tf.float32)
    
    # ê¸°ë³¸ MSE
    mse_per_step = tf.reduce_mean(tf.square(y_true - y_pred) + epsilon, axis=0)
    
    # ê³¼ëŒ€ ì˜ˆì¸¡ íŒ¨ë„í‹° (ì²«ë‚  ê°•í™”)
    overprediction_penalty = tf.reduce_mean(
        tf.maximum(0.0, y_pred - y_true) * tf.constant([25.0, 15.0, 10.0, 8.0, 5.0], dtype=tf.float32)
    )
    
    # ê³¼ì†Œ ì˜ˆì¸¡ íŒ¨ë„í‹° (ì²«ë‚  ê°•í™”)
    underprediction_penalty = tf.reduce_mean(
        tf.maximum(0.0, y_true - y_pred) * tf.constant([15.0, 8.0, 6.0, 4.0, 3.0], dtype=tf.float32)
    )
    
    # ì¶”ì„¸ ì†ì‹¤ (ì²«ë‚  ê°•í™”)
    y_true_diff = y_true[:, 1:] - y_true[:, :-1]
    y_pred_diff = y_pred[:, 1:] - y_pred[:, :-1]
    trend_weights = tf.constant([0.5, 0.3, 0.15, 0.05], dtype=tf.float32)
    trend_loss = tf.reduce_mean(tf.square(y_true_diff - y_pred_diff) * trend_weights + epsilon)
    
    # ë°©í–¥ì„± ì†ì‹¤ (ì²«ë‚  ê°•í™”)
    direction_weights = tf.constant([0.5, 0.3, 0.15, 0.05], dtype=tf.float32)
    direction_loss = tf.reduce_mean(
        tf.square(tf.sign(y_true_diff) - tf.sign(y_pred_diff)) * direction_weights + epsilon
    )
    
    # ê°€ì¤‘ì¹˜ ì ìš©
    weighted_loss = (
        tf.reduce_sum(mse_per_step * time_weights) +
        0.7 * overprediction_penalty +
        0.5 * underprediction_penalty +
        0.4 * trend_loss +
        0.3 * direction_loss
    )
    return weighted_loss

# ë°ì´í„° ì¦ê°• í•¨ìˆ˜ ê°œì„ 
def augment_data(X, y, noise_level=0.01):
    """ë°ì´í„°ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•˜ì—¬ ì¦ê°•"""
    X_aug = X.copy()
    y_aug = y.copy()
    
    # ê°€ìš°ì‹œì•ˆ ë…¸ì´ì¦ˆ ì¶”ê°€
    noise = np.random.normal(0, noise_level, X.shape)
    X_aug = X_aug + noise
    
    # ì‹œê³„ì—´ íŠ¹ì„± ë³´ì¡´ì„ ìœ„í•œ ë…¸ì´ì¦ˆ ì œí•œ
    X_aug = np.clip(X_aug, X.min(), X.max())
    
    return X_aug, y_aug

# ëª¨ë¸ êµ¬ì¡° ê°œì„ 
def build_enhanced_model(input_shape, output_days=5):
    inputs = Input(shape=input_shape)
    
    # 1. ì…ë ¥ ì •ê·œí™” ë ˆì´ì–´
    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(inputs)
    x = Dropout(0.2)(x)
    
    # 2. Conv1D ë ˆì´ì–´
    x = Conv1D(64, 3, padding='same', activation='relu')(x)
    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)
    x = Dropout(0.2)(x)
    
    # 3. LSTM ë ˆì´ì–´
    x = tf.keras.layers.Bidirectional(
        tf.keras.layers.LSTM(64, return_sequences=True, recurrent_dropout=0.1)
    )(x)
    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)
    x = Dropout(0.2)(x)
    
    # 4. Attention ë©”ì»¤ë‹ˆì¦˜
    attention = MultiHeadAttention(num_heads=8, key_dim=64)(x, x)
    x = tf.keras.layers.Add()([x, attention])
    x = BatchNormalization(momentum=0.9, epsilon=1e-5)(x)
    x = Dropout(0.2)(x)
    
    # 5. ì¶œë ¥ ë ˆì´ì–´
    outputs = []
    for i in range(output_days):
        day_output = TimeDistributed(Dense(32, activation='relu'))(x)
        day_output = BatchNormalization(momentum=0.9, epsilon=1e-5)(day_output)
        day_output = Dropout(0.2)(day_output)
        day_output = Dense(1, activation='sigmoid', name=f'day_{i+1}_output')(day_output[:, -1, :])  # sigmoid í™œì„±í™” í•¨ìˆ˜ ì‚¬ìš©
        outputs.append(day_output)
    
    final_output = tf.keras.layers.Concatenate()(outputs)
    
    # ëª¨ë¸ ìƒì„±
    model = Model(inputs=inputs, outputs=final_output)
    
    # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ë§
    initial_learning_rate = 0.0001
    
    optimizer = AdamW(
        learning_rate=initial_learning_rate,
        weight_decay=0.01,
        beta_1=0.9,
        beta_2=0.999,
        epsilon=1e-07
    )
    
    model.compile(
        optimizer=optimizer,
        loss=enhanced_weighted_time_mse,
        metrics=['mae', 'mse']
    )
    
    return model

# í•™ìŠµ ê³¼ì • ê°œì„ 
def train_enhanced_model(model, X_train, y_train, X_test, y_test):
    # ì½œë°± ì •ì˜
    callbacks = [
        EarlyStopping(
            monitor='val_loss',
            patience=15,
            restore_best_weights=True,
            min_delta=0.0001
        ),
        ReduceLROnPlateau(
            monitor='val_loss',
            factor=0.5,
            patience=5,
            min_lr=1e-7,
            min_delta=0.0001
        ),
        tf.keras.callbacks.ModelCheckpoint(
            '/kaggle/working/best_model.keras',
            monitor='val_loss',
            save_best_only=True,
            save_weights_only=False
        ),
        tf.keras.callbacks.LearningRateScheduler(
            lambda epoch: 0.0001 * (0.95 ** (epoch // 3))
        )
    ]
    
    # ë°ì´í„° ì¦ê°• ì ìš©
    X_train_aug, y_train_aug = augment_data(X_train, y_train, noise_level=0.01)
    
    # í•™ìŠµ
    history = model.fit(
        X_train_aug, y_train_aug,
        validation_data=(X_test, y_test),
        epochs=150,
        batch_size=BATCH_SIZE,
        callbacks=callbacks,
        verbose=1,
        shuffle=True
    )
    
    return history

# ë°ì´í„° ë¶„ì„
print("\n[ë°ì´í„° ë¶„ì„]")
# ìµœê·¼ 30ì¼ ë°ì´í„° ìš”ì•½ í†µê³„
recent_data = merged_data.tail(30).describe()
print("ìµœê·¼ 30ì¼ ë°ì´í„° ìš”ì•½:\n", recent_data['í˜„ì¬ê°€'])

# ë°ì´í„° í¬ê¸° í™•ì¸
print(f"\nìµœì¢… ë°ì´í„° í¬ê¸°: {merged_data.shape}")

# 2. íŠ¹ì„± ì„ íƒ ë° ìŠ¤ì¼€ì¼ë§
# ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì‚¬ìš©
price_features = ['ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'í˜„ì¬ê°€', 'ê±°ë˜ëŸ‰']
sentiment_features = [
    'finbert_positive', 'finbert_negative', 'finbert_neutral'
]
economic_features = [
    'treasury_10y', 'dollar_index', 'usd_krw', 'korean_bond_10y'
]
technical_features = [
    'RSI', 'MACD', 'MACD_SIGNAL', 'MACD_HIST',
    'BB_UPPER', 'BB_MIDDLE', 'BB_LOWER', 'BB_PERCENT',
    'MA5', 'MA20', 'MA60',
    'VOLUME_MA5', 'VOLUME_MA20', 'VOLUME_RATIO',
    'MOM', 'ROC'
]

# ë‹¨ê¸° ì˜ˆì¸¡ì„ ìœ„í•œ íŠ¹ì„±ê³¼ ì¥ê¸° ì˜ˆì¸¡ì„ ìœ„í•œ íŠ¹ì„± êµ¬ë¶„
short_term_features = price_features + [
    'RSI', 'VOLUME_RATIO', 'MOM', 'ROC',
    'MA5', 'BB_PERCENT', 'MACD',
    'treasury_10y', 'usd_krw'  # ë‹¨ê¸° ì˜ˆì¸¡ì— ì¤‘ìš”í•œ ê²½ì œì§€í‘œ
]

long_term_features = price_features + [
    'MA20', 'MA60', 'BB_PERCENT',
    'MACD', 'MACD_HIST', 'VOLUME_MA20',
    'treasury_10y', 'dollar_index', 'korean_bond_10y'  # ì¥ê¸° ì˜ˆì¸¡ì— ì¤‘ìš”í•œ ê²½ì œì§€í‘œ
]

# ê³µí†µ íŠ¹ì„±
all_features = list(set(short_term_features + long_term_features))

# ìŠ¤ì¼€ì¼ë§ ì „ì— ë°ì´í„° í™•ì¸
print("\n[ìŠ¤ì¼€ì¼ë§ ì „ ë°ì´í„° í™•ì¸]")
print("ì‚¬ìš©í•  ëª¨ë“  íŠ¹ì„± ê°œìˆ˜:", len(all_features))
print("ë‹¨ê¸° ì˜ˆì¸¡ íŠ¹ì„± ê°œìˆ˜:", len(short_term_features))
print("ì¥ê¸° ì˜ˆì¸¡ íŠ¹ì„± ê°œìˆ˜:", len(long_term_features))

# ë°ì´í„° íƒ€ì… í™•ì¸
print("\n[ë°ì´í„° íƒ€ì… í™•ì¸]")
print(merged_data[all_features].dtypes)

# ì •ìƒ ë²”ìœ„ê°€ ì•„ë‹Œ ê°’ í™•ì¸
print("\n[ë¹„ì •ìƒì ì¸ ê°’ í™•ì¸]")
for col in all_features:
    non_finite = (~np.isfinite(merged_data[col])).sum()
    if non_finite > 0:
        print(f"- {col}: {non_finite}ê°œì˜ ë¹„ì •ìƒì ì¸ ê°’ (inf, -inf, NaN)")
        # ë¹„ì •ìƒì ì¸ ê°’ì„ 0ìœ¼ë¡œ ëŒ€ì²´
        merged_data[col] = np.nan_to_num(merged_data[col], nan=0.0, posinf=0.0, neginf=0.0)

# ìŠ¤ì¼€ì¼ë§ ë° ì—­ìŠ¤ì¼€ì¼ë§ì„ ìœ„í•œ í´ë˜ìŠ¤ ìˆ˜ì •
class EnhancedPriceScaler:
    def __init__(self):
        self.price_scaler = MinMaxScaler(feature_range=(0.1, 0.9))  # 0-1 ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ì§€ ì•Šë„ë¡ ì¡°ì •
        self.feature_scaler = MinMaxScaler(feature_range=(0.1, 0.9))

    def fit_transform(self, data, price_cols):
        data_copy = data.copy()
        
        # ë¬¸ìì—´ ì»¬ëŸ¼ê³¼ ë‚ ì§œ ì»¬ëŸ¼ ì œì™¸
        exclude_cols = ['ê¸°ì¤€ì¼ì', 'ì¢…ëª©ì½”ë“œ', 'ì¢…ëª©ëª…', 'Title', 'PubDate', 'finbert_sentiment']
        for col in exclude_cols:
            if col in data_copy.columns:
                data_copy = data_copy.drop(columns=[col])
        
        # ê°€ê²© ë°ì´í„°ì™€ ë‹¤ë¥¸ íŠ¹ì„± ë¶„ë¦¬
        price_data = data_copy[price_cols]
        other_data = data_copy.drop(columns=price_cols)
        
        # ê°ê° ìŠ¤ì¼€ì¼ë§
        scaled_price = self.price_scaler.fit_transform(price_data)
        scaled_other = self.feature_scaler.fit_transform(other_data)
        
        # ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ê²°í•©
        scaled_data = np.concatenate([scaled_price, scaled_other], axis=1)
        scaled_df = pd.DataFrame(scaled_data, columns=price_cols + other_data.columns.tolist())
        
        # ì›ë˜ ì»¬ëŸ¼ ìˆœì„œ ë³µì›
        scaled_df = scaled_df[data_copy.columns]
        
        return scaled_df

    def inverse_transform_price(self, scaled_price):
        if len(scaled_price.shape) == 1:
            scaled_price = scaled_price.reshape(-1, 1)
        
        dummy_data = np.zeros((scaled_price.shape[0], len(self.price_scaler.feature_names_in_)))
        dummy_data[:, 0] = scaled_price.flatten()
        
        unscaled = self.price_scaler.inverse_transform(dummy_data)
        return unscaled[:, 0]

# ë°ì´í„° ì „ì²˜ë¦¬ ìˆ˜ì •
try:
    # ë¬¸ìì—´ ì»¬ëŸ¼ê³¼ ë‚ ì§œ ì»¬ëŸ¼ ì €ì¥
    exclude_cols = ['ê¸°ì¤€ì¼ì', 'ì¢…ëª©ì½”ë“œ', 'ì¢…ëª©ëª…', 'Title', 'PubDate', 'finbert_sentiment']
    excluded_data = merged_data[exclude_cols].copy()

    # ìŠ¤ì¼€ì¼ë§í•  ì»¬ëŸ¼ ì„ íƒ
    price_cols = ['í˜„ì¬ê°€', 'ì‹œê°€', 'ê³ ê°€', 'ì €ê°€']
    other_cols = [col for col in merged_data.columns if col not in price_cols + exclude_cols]

    # ìŠ¤ì¼€ì¼ëŸ¬ ì´ˆê¸°í™” ë° ì ìš©
    scaler = EnhancedPriceScaler()
    data_scaled = scaler.fit_transform(merged_data, price_cols)

    # ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„°ë¥¼ DataFrameìœ¼ë¡œ ë³€í™˜
    scaled_df = pd.DataFrame(data_scaled, columns=price_cols + other_cols)

    # ì œì™¸ëœ ì»¬ëŸ¼ ë‹¤ì‹œ ì¶”ê°€
    for col in exclude_cols:
        scaled_df[col] = excluded_data[col]

    print("\n[ìŠ¤ì¼€ì¼ë§ í›„ ë°ì´í„° í™•ì¸]")
    print("ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° í¬ê¸°:", scaled_df.shape)
    print("ìŠ¤ì¼€ì¼ë§ëœ ë°ì´í„° ìƒ˜í”Œ:\n", scaled_df.head())

    # ì‹œí€€ìŠ¤ ìƒì„±
    window_size = 110
    future_days = 5

    # ë°ì´í„°ê°€ ì¶©ë¶„í•œì§€ í™•ì¸
    if len(scaled_df) > window_size + future_days:
        # íŠ¹ì„± ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ìƒì„±
        feature_names = price_cols + other_cols

        # numpy ë°°ì—´ë¡œ ë³€í™˜
        data_array = scaled_df[feature_names].values

        # ì‹œí€€ìŠ¤ ìƒì„±
        sequences = []
        targets = []
        dates = []

        # í˜„ì¬ê°€ ì»¬ëŸ¼ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
        current_price_idx = feature_names.index('í˜„ì¬ê°€')

        for i in range(len(data_array) - window_size - future_days + 1):
            # ì…ë ¥ ì‹œí€€ìŠ¤
            sequence = data_array[i:(i + window_size)]
            sequences.append(sequence)

            # íƒ€ê²Ÿ (ë‹¤ìŒ 5ì¼ì˜ í˜„ì¬ê°€)
            target = data_array[(i + window_size):(i + window_size + future_days), current_price_idx]
            targets.append(target)

            # ë‚ ì§œ ì •ë³´ ì €ì¥
            dates.append(scaled_df.index[i + window_size])

        X = np.array(sequences)
        y = np.array(targets)

        print(f"ìƒì„±ëœ ì‹œí€€ìŠ¤ í¬ê¸°: {X.shape}")
        print(f"ìƒì„±ëœ íƒ€ê²Ÿ í¬ê¸°: {y.shape}")

        # 4. ë°ì´í„° ë¶„í• 
        split_idx = int(len(X) * 0.8)  # 80% í›ˆë ¨, 20% í…ŒìŠ¤íŠ¸

        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]
        dates_train, dates_test = dates[:split_idx], dates[split_idx:]

        print(f"í›ˆë ¨ ë°ì´í„° í¬ê¸°: X_train={X_train.shape}, y_train={y_train.shape}")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: X_test={X_test.shape}, y_test={y_test.shape}")
    else:
        print(f"ì˜¤ë¥˜: ì‹œí€€ìŠ¤ ìƒì„±ì„ ìœ„í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. í•„ìš”: {window_size + future_days}ì¼, í˜„ì¬: {len(scaled_df)}ì¼")
        print("ëª¨ë¸ í›ˆë ¨ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        raise ValueError("ë°ì´í„° ë¶€ì¡±")

except Exception as e:
    print(f"ë°ì´í„° ì „ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    raise

# 3. ê°œì„ ëœ ì‹œí€€ìŠ¤ ìƒì„± í•¨ìˆ˜
def create_sequences(data, feat_names, window_size=110, future_days=5):
    """
    ì‹œê³„ì—´ ë°ì´í„°ì—ì„œ ì‹œí€€ìŠ¤ì™€ íƒ€ê²Ÿ ìƒì„±
    """
    sequences = []
    targets = []
    dates = []

    # í˜„ì¬ê°€ ì»¬ëŸ¼ì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
    current_price_idx = feat_names.index('í˜„ì¬ê°€') if 'í˜„ì¬ê°€' in feat_names else None

    if current_price_idx is None:
        raise ValueError("'í˜„ì¬ê°€' íŠ¹ì„±ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # ë°ì´í„°ë¥¼ numpy ë°°ì—´ë¡œ ë³€í™˜
    data_array = data[feat_names].values if isinstance(data, pd.DataFrame) else data

    for i in range(len(data_array) - window_size - future_days + 1):
        # ì…ë ¥ ì‹œí€€ìŠ¤
        sequence = data_array[i:(i + window_size)]
        sequences.append(sequence)

        # íƒ€ê²Ÿ (ë‹¤ìŒ 5ì¼ì˜ í˜„ì¬ê°€)
        target = data_array[(i + window_size):(i + window_size + future_days), current_price_idx]
        targets.append(target)

        # ë‚ ì§œ ì •ë³´ ì €ì¥
        if isinstance(data, pd.DataFrame):
            dates.append(data.index[i + window_size])
        else:
            dates.append(i + window_size)

    sequences = np.array(sequences)
    targets = np.array(targets)

    print(f"ìƒì„±ëœ ì‹œí€€ìŠ¤ í¬ê¸°: {sequences.shape}")
    print(f"ìƒì„±ëœ íƒ€ê²Ÿ í¬ê¸°: {targets.shape}")

    # NaN ë˜ëŠ” ë¬´í•œëŒ€ ê°’ í™•ì¸
    if np.isnan(sequences).any() or np.isinf(sequences).any():
        print("ê²½ê³ : ì‹œí€€ìŠ¤ì— NaN ë˜ëŠ” ë¬´í•œëŒ€ ê°’ì´ ìˆìŠµë‹ˆë‹¤.")
        sequences = np.nan_to_num(sequences, nan=0.0, posinf=1.0, neginf=0.0)

    if np.isnan(targets).any() or np.isinf(targets).any():
        print("ê²½ê³ : íƒ€ê²Ÿì— NaN ë˜ëŠ” ë¬´í•œëŒ€ ê°’ì´ ìˆìŠµë‹ˆë‹¤.")
        targets = np.nan_to_num(targets, nan=0.0, posinf=1.0, neginf=0.0)

    return sequences, targets, dates

# ê°œì„  9: íŠ¹ì„± ì¸ë±ìŠ¤ ë§¤í•‘ ìƒì„±
def create_feature_indices(all_feature_names, short_term_features, long_term_features):
    """íŠ¹ì„± ì´ë¦„ì— ë”°ë¥¸ ì¸ë±ìŠ¤ ë§¤í•‘ ìƒì„±"""
    short_term_indices = [all_feature_names.index(f) for f in short_term_features if f in all_feature_names]
    long_term_indices = [all_feature_names.index(f) for f in long_term_features if f in all_feature_names]

    # ê³µí†µ ì¸ë±ìŠ¤ëŠ” ë‹¨ê¸°ì™€ ì¥ê¸° ëª¨ë‘ì— í¬í•¨ë¨
    common_indices = list(set(short_term_indices) & set(long_term_indices))

    return {
        'short_term': short_term_indices,
        'long_term': long_term_indices,
        'common': common_indices
    }

# íŠ¹ì„± ì¸ë±ìŠ¤ ë§¤í•‘ ìƒì„±
feature_indices = create_feature_indices(all_features, short_term_features, long_term_features)

print("\n[íŠ¹ì„± ì¸ë±ìŠ¤ ì •ë³´]")
print(f"ë‹¨ê¸° ì˜ˆì¸¡ íŠ¹ì„± ì¸ë±ìŠ¤ ìˆ˜: {len(feature_indices['short_term'])}")
print(f"ì¥ê¸° ì˜ˆì¸¡ íŠ¹ì„± ì¸ë±ìŠ¤ ìˆ˜: {len(feature_indices['long_term'])}")
print(f"ê³µí†µ íŠ¹ì„± ì¸ë±ìŠ¤ ìˆ˜: {len(feature_indices['common'])}")

# MCI-GRU ì…€ êµ¬í˜„
class MCI_GRU_Cell(Layer):
    def __init__(self, units, num_heads=4, **kwargs):
        super(MCI_GRU_Cell, self).__init__(**kwargs)
        self.units = units
        self.num_heads = num_heads
        self.state_size = units  # state_size ì†ì„± ì¶”ê°€
        self.output_size = units  # output_size ì†ì„± ì¶”ê°€

    def build(self, input_shape):
        input_dim = input_shape[-1]

        # ì…ë ¥ íˆ¬ì˜ ê°€ì¤‘ì¹˜ (ì…ë ¥ ì°¨ì›ì„ units ì°¨ì›ìœ¼ë¡œ ë³€í™˜)
        self.W_input = self.add_weight(shape=(input_dim, self.units),
                                     name='W_input',
                                     initializer='glorot_uniform')

        # GRU ê²Œì´íŠ¸ ê°€ì¤‘ì¹˜
        self.W_z = self.add_weight(shape=(self.units, self.units),
                                 name='W_z',
                                 initializer='glorot_uniform')
        self.U_z = self.add_weight(shape=(self.units, self.units),
                                 name='U_z',
                                 initializer='glorot_uniform')
        self.b_z = self.add_weight(shape=(self.units,),
                                 name='b_z',
                                 initializer='zeros')

        self.W_r = self.add_weight(shape=(self.units, self.units),
                                 name='W_r',
                                 initializer='glorot_uniform')
        self.U_r = self.add_weight(shape=(self.units, self.units),
                                 name='U_r',
                                 initializer='glorot_uniform')
        self.b_r = self.add_weight(shape=(self.units,),
                                 name='b_r',
                                 initializer='zeros')

        self.W_h = self.add_weight(shape=(self.units, self.units),
                                 name='W_h',
                                 initializer='glorot_uniform')
        self.U_h = self.add_weight(shape=(self.units, self.units),
                                 name='U_h',
                                 initializer='glorot_uniform')
        self.b_h = self.add_weight(shape=(self.units,),
                                 name='b_h',
                                 initializer='zeros')

        # Attention ê°€ì¤‘ì¹˜
        self.W_q = self.add_weight(shape=(self.units, self.units),
                                 name='W_q',
                                 initializer='glorot_uniform')
        self.W_k = self.add_weight(shape=(self.units, self.units),
                                 name='W_k',
                                 initializer='glorot_uniform')
        self.W_v = self.add_weight(shape=(self.units, self.units),
                                 name='W_v',
                                 initializer='glorot_uniform')

        self.built = True

    def call(self, inputs, states):
        h_prev = states[0]  # ì´ì „ ìƒíƒœ

        # ì…ë ¥ íˆ¬ì˜
        x = tf.matmul(inputs, self.W_input)

        # Multi-head self-attention
        q = tf.matmul(h_prev, self.W_q)
        k = tf.matmul(h_prev, self.W_k)
        v = tf.matmul(h_prev, self.W_v)

        # Scaled dot-product attention
        scale = tf.math.sqrt(tf.cast(self.units, tf.float32))
        attention_scores = tf.matmul(q, k, transpose_b=True) / scale
        attention_weights = tf.nn.softmax(attention_scores)
        context = tf.matmul(attention_weights, v)

        # GRU gates with attention
        z = tf.sigmoid(tf.matmul(x, self.W_z) + tf.matmul(context, self.U_z) + self.b_z)
        r = tf.sigmoid(tf.matmul(x, self.W_r) + tf.matmul(context, self.U_r) + self.b_r)
        h_tilde = tf.tanh(tf.matmul(x, self.W_h) + tf.matmul(r * context, self.U_h) + self.b_h)
        h = (1 - z) * context + z * h_tilde

        return h, [h]

    def get_config(self):
        config = super(MCI_GRU_Cell, self).get_config()
        config.update({
            'units': self.units,
            'num_heads': self.num_heads
        })
        return config

# ì•™ìƒë¸” ëª¨ë¸ í´ë˜ìŠ¤ ê°œì„ 
class EnsembleModel:
    def __init__(self, input_shape, n_models=3):
        self.models = []
        self.input_shape = input_shape
        self.n_models = n_models
        self.model_weights = None
        
    def build_models(self):
        for i in range(self.n_models):
            model = build_enhanced_model(self.input_shape)
            self.models.append(model)
    
    def train(self, X_train, y_train, X_val, y_val):
        histories = []
        val_losses = []
        
        # ê° ëª¨ë¸ í•™ìŠµ
        for i, model in enumerate(self.models):
            print(f"\nTraining model {i+1}/{self.n_models}")
            
            # ë°ì´í„° ì¦ê°• ê°•í™”
            X_train_aug, y_train_aug = augment_data(X_train, y_train, noise_level=0.01 * (i + 1))
            
            history = train_enhanced_model(model, X_train_aug, y_train_aug, X_val, y_val)
            histories.append(history)
            
            # ê²€ì¦ ì†ì‹¤ ì €ì¥
            val_loss = min(history.history['val_loss'])
            val_losses.append(val_loss)
        
        # ëª¨ë¸ ê°€ì¤‘ì¹˜ ê³„ì‚° (ê²€ì¦ ì†ì‹¤ ê¸°ë°˜)
        val_losses = np.array(val_losses)
        self.model_weights = 1.0 / (val_losses + 1e-7)
        self.model_weights = self.model_weights / np.sum(self.model_weights)
        
        return histories
    
    def predict(self, X):
        predictions = []
        for i, model in enumerate(self.models):
            pred = model.predict(X)
            predictions.append(pred * self.model_weights[i])
        return np.sum(predictions, axis=0)

# ì•™ìƒë¸” ëª¨ë¸ ì‚¬ìš©
ensemble = EnsembleModel(input_shape=(X_train.shape[1], X_train.shape[2]))
ensemble.build_models()
histories = ensemble.train(X_train, y_train, X_test, y_test)

# ì˜ˆì¸¡ ìˆ˜í–‰
predictions = ensemble.predict(X_test)

# í•™ìŠµ ê²°ê³¼ ì‹œê°í™”
plt.figure(figsize=(12, 4))

plt.subplot(1, 2, 1)
plt.plot(histories[0].history['loss'], label='Training Loss')
plt.plot(histories[0].history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(histories[0].history['mae'], label='Training MAE')
plt.plot(histories[0].history['val_mae'], label='Validation MAE')
plt.title('Model MAE')
plt.xlabel('Epoch')
plt.ylabel('MAE')
plt.legend()

plt.tight_layout()
plt.show()

# ëª¨ë¸ ì €ì¥
for i, model in enumerate(ensemble.models):
    model.save(f'/kaggle/working/stock_prediction_model_{i+1}.keras')  # .h5 ëŒ€ì‹  .keras ì‚¬ìš©
    
# ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
with open(f'/kaggle/working/stock_prediction_scaler_{i+1}.pkl', 'wb') as f:
    pickle.dump(scaler, f)
    
# ëª¨ë¸ ë©”íƒ€ë°ì´í„° ì €ì¥
model_metadata = {
    'input_shape': X_train.shape[1:],
    'output_days': 5,
    'price_scaler_params': {
        'scale_': scaler.price_scaler.scale_.tolist(),
        'min_': scaler.price_scaler.min_.tolist(),  # center_ ëŒ€ì‹  min_ ì‚¬ìš©
        'data_min_': scaler.price_scaler.data_min_.tolist(),
        'data_max_': scaler.price_scaler.data_max_.tolist(),
        'data_range_': scaler.price_scaler.data_range_.tolist()
    }
}
    
with open(f'/kaggle/working/model_metadata_{i+1}.json', 'w') as f:
    json.dump(model_metadata, f)
        
print(f"ëª¨ë¸ {i+1} ì €ì¥ ì™„ë£Œ")
print(f"ìŠ¤ì¼€ì¼ëŸ¬ {i+1} ì €ì¥ ì™„ë£Œ")
print(f"ë©”íƒ€ë°ì´í„° {i+1} ì €ì¥ ì™„ë£Œ")

# ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”
try:
    # í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ìˆ˜í–‰
    predictions = ensemble.predict(X_test)

    # ë§ˆì§€ë§‰ ì˜ˆì¸¡ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸° (ê°€ì¥ ìµœê·¼ ì˜ˆì¸¡)
    last_prediction = predictions[-1]

    # ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì›ë˜ ìŠ¤ì¼€ì¼ë¡œ ë³€í™˜
    last_prediction = scaler.inverse_transform_price(last_prediction.reshape(-1, 1)).flatten()

    # ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ ê°’ ë¹„êµ
    target_dates = ['2025-03-24', '2025-03-25', '2025-03-26', '2025-03-27', '2025-03-28']
    target_prices = [69700, 67500, 67200, 66800, 65700]

    # ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”
    plt.figure(figsize=(12, 6))

    # ë‚ ì§œë¥¼ datetime ê°ì²´ë¡œ ë³€í™˜
    dates = [datetime.strptime(date, '%Y-%m-%d') for date in target_dates]

    # ì‹¤ì œ ê°€ê²©ê³¼ ì˜ˆì¸¡ ê°€ê²© í”Œë¡¯
    plt.plot(dates, target_prices, 'b-', label='ì‹¤ì œ ê°€ê²©', marker='o')
    plt.plot(dates, last_prediction, 'r--', label='ì˜ˆì¸¡ ê°€ê²©', marker='s')

    # ê·¸ë˜í”„ ìŠ¤íƒ€ì¼ë§
    plt.title('LGì „ì ì£¼ê°€ ì˜ˆì¸¡ ê²°ê³¼ (2025ë…„ 3ì›”)', fontsize=14)
    plt.xlabel('ë‚ ì§œ', fontsize=12)
    plt.ylabel('ì£¼ê°€ (ì›)', fontsize=12)
    plt.grid(True, linestyle='--', alpha=0.7)
    plt.legend(fontsize=12)

    # xì¶• ë‚ ì§œ í¬ë§· ì„¤ì •
    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    plt.gca().xaxis.set_major_locator(mdates.DayLocator())
    plt.xticks(rotation=45)

    # ì˜¤ì°¨ìœ¨ ê³„ì‚° ë° í‘œì‹œ
    error_rates = [(pred - actual) / actual * 100 for pred, actual in zip(last_prediction, target_prices)]
    for i, (date, error) in enumerate(zip(dates, error_rates)):
        plt.annotate(f'{error:.2f}%',
                    xy=(date, max(last_prediction[i], target_prices[i])),
                    xytext=(0, 10),
                    textcoords='offset points',
                    ha='center',
                    fontsize=10)

    plt.tight_layout()
    plt.show()

    # ì˜ˆì¸¡ ê²°ê³¼ ìƒì„¸ ë¶„ì„
    print("\n[ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„]")
    print(f"{'ë‚ ì§œ':<12} {'ì‹¤ì œ ê°€ê²©':>10} {'ì˜ˆì¸¡ ê°€ê²©':>10} {'ì˜¤ì°¨ìœ¨':>8}")
    print("-" * 45)
    for date, actual, pred, error in zip(target_dates, target_prices, last_prediction, error_rates):
        print(f"{date:<12} {actual:>10,d} {pred:>10.0f} {error:>7.2f}%")

    # ì „ì²´ ì˜ˆì¸¡ ì„±ëŠ¥ ì§€í‘œ
    mae = mean_absolute_error(target_prices, last_prediction)
    mse = mean_squared_error(target_prices, last_prediction)
    rmse = np.sqrt(mse)
    mape = np.mean(np.abs(error_rates))

    print("\n[ì „ì²´ ì˜ˆì¸¡ ì„±ëŠ¥]")
    print(f"MAE: {mae:.2f}")
    print(f"RMSE: {rmse:.2f}")
    print(f"MAPE: {mape:.2f}%")

    # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
    for i, (date, pred, actual) in enumerate(zip(target_dates, last_prediction, target_prices)):
        save_prediction(
            stock_code='066570',  # LGì „ì ì¢…ëª©ì½”ë“œ
            stock_name='LGì „ì',
            prediction_date=datetime.now(),
            target_date=datetime.strptime(date, '%Y-%m-%d'),
            predicted_price=pred,
            actual_price=actual
        )
    
    print("âœ… ì˜ˆì¸¡ ê²°ê³¼ê°€ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

except Exception as e:
    print(f"ì˜ˆì¸¡ ê²°ê³¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    raise

if __name__ == "__main__":
    print("ğŸ“¢ ì£¼ê°€ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤...")
    create_predictions_table()
    
    # ë°ì´í„° ë¡œë“œ
    stock_data, sentiment_data, economic_data = load_data_from_db()
    
    # ë°ì´í„° ì „ì²˜ë¦¬ ë° ëª¨ë¸ í•™ìŠµ
    # ... (ê¸°ì¡´ ì½”ë“œ ìœ ì§€) ...
    
    # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥
    for i, (date, pred, actual) in enumerate(zip(target_dates, last_prediction, target_prices)):
        save_prediction(
            stock_code='066570',  # LGì „ì ì¢…ëª©ì½”ë“œ
            stock_name='LGì „ì',
            prediction_date=datetime.now(),
            target_date=datetime.strptime(date, '%Y-%m-%d'),
            predicted_price=pred,
            actual_price=actual
        )
    
    print("âœ… ì˜ˆì¸¡ ê²°ê³¼ê°€ ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")